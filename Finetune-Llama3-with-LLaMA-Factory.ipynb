{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oP6e5vmz-xB9-mttIJSDO0vr1HPQZZbC","timestamp":1718264128398},{"file_id":"1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9","timestamp":1715878308057}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Finetune Llama-3 with LLaMA Factory\n","\n","Please use a **free** Tesla T4 Colab GPU to run this!\n","\n","Project homepage: https://github.com/hiyouga/LLaMA-Factory"],"metadata":{"id":"1oHFCsV0z-Jw"}},{"cell_type":"markdown","source":["## Install Dependencies"],"metadata":{"id":"lr7rB3szzhtx"}},{"cell_type":"code","source":["# mounting drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd drive/MyDrive/BullingerDigitalLMFootnotes/git_repo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvCM3AkWaIkk","executionInfo":{"status":"ok","timestamp":1718264559879,"user_tz":-120,"elapsed":21918,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"82949666-efcf-4a95-9016-4cd320267f8d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/BullingerDigitalLMFootnotes/git_repo\n"]}]},{"cell_type":"code","source":["# !git clone https://github.com/hiyouga/LLaMA-Factory.git  # clone the repo\n","# %rm -rf ../LLaMA-Factory/  # remove the old version (!!!!! also removes added data files)\n","# %mv LLaMA-Factory/ ..  # move folder one up, out of the git repo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cstu5_Ngi_sS","executionInfo":{"status":"ok","timestamp":1718264591298,"user_tz":-120,"elapsed":25979,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"a62adf2e-0ea6-48ab-d1f9-aa4c555e2b4e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'LLaMA-Factory'...\n","remote: Enumerating objects: 13541, done.\u001b[K\n","remote: Counting objects: 100% (468/468), done.\u001b[K\n","remote: Compressing objects: 100% (212/212), done.\u001b[K\n","remote: Total 13541 (delta 279), reused 365 (delta 239), pack-reused 13073\u001b[K\n","Receiving objects: 100% (13541/13541), 219.80 MiB | 13.32 MiB/s, done.\n","Resolving deltas: 100% (9893/9893), done.\n","Updating files: 100% (217/217), done.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giM74oK1rRIH","executionInfo":{"status":"ok","timestamp":1715889798192,"user_tz":-120,"elapsed":161812,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"3abbafad-be37-4d4a-e1bf-6010bd3f818b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP_2_Conspiracies/code/task/LM_classification_huggingface/fine_tune_llama/LLaMA-Factory\n","\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdata\u001b[0m/               LICENSE            README_zh.md      \u001b[01;34msrc\u001b[0m/\n","\u001b[01;34mbuild\u001b[0m/        docker-compose.yml  Makefile           requirements.txt  \u001b[01;34mtests\u001b[0m/\n","\u001b[01;34mcache\u001b[0m/        Dockerfile          merge_llama3.json  \u001b[01;34msaves\u001b[0m/            train_llama3.json\n","CITATION.cff  \u001b[01;34mevaluation\u001b[0m/         pyproject.toml     \u001b[01;34mscripts\u001b[0m/\n","\u001b[01;34mconfig\u001b[0m/       \u001b[01;34mexamples\u001b[0m/           README.md          setup.py\n","Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-rzmgeld4/unsloth_33ac51ebd6e247e1b9487433c5452855\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-rzmgeld4/unsloth_33ac51ebd6e247e1b9487433c5452855\n","  Resolved https://github.com/unslothai/unsloth.git to commit 8dc0561ec0776fcc49d8a406c8a0acf295bd561a\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.40.2)\n","Collecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n","Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n","Collecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n","Collecting huggingface-hub>=0.21.2 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n","Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n","Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n","Building wheels for collected packages: unsloth\n","  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unsloth: filename=unsloth-2024.5-py3-none-any.whl size=105043 sha256=5e507bada1e955e3d45aced9d2696b493eb60e27d964ec26cf0b7cb88961f297\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9jpfzt_n/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n","Successfully built unsloth\n","Installing collected packages: xxhash, unsloth, shtab, dill, multiprocess, huggingface-hub, tyro, datasets\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 shtab-1.7.1 tyro-0.8.4 unsloth-2024.5 xxhash-3.4.1\n","/bin/bash: line 1: 0.0.26: No such file or directory\n","Processing /content/drive/MyDrive/NLP_2_Conspiracies/code/task/LM_classification_huggingface/fine_tune_llama/LLaMA-Factory\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (4.40.2)\n","Requirement already satisfied: datasets>=2.14.3 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.19.1)\n","Collecting accelerate>=0.27.2 (from llamafactory==0.7.2.dev0)\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft>=0.10.0 (from llamafactory==0.7.2.dev0)\n","  Downloading peft-0.11.0-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl>=0.8.1 (from llamafactory==0.7.2.dev0)\n","  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gradio>=4.0.0 (from llamafactory==0.7.2.dev0)\n","  Downloading gradio-4.31.3-py3-none-any.whl (12.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (1.11.4)\n","Collecting einops (from llamafactory==0.7.2.dev0)\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.20.3)\n","Collecting uvicorn (from llamafactory==0.7.2.dev0)\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.7.1)\n","Collecting fastapi (from llamafactory==0.7.2.dev0)\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sse-starlette (from llamafactory==0.7.2.dev0)\n","  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n","Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.7.1)\n","Collecting fire (from llamafactory==0.7.2.dev0)\n","  Downloading fire-0.6.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (24.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (6.0.1)\n","Collecting bitsandbytes>=0.39.0 (from llamafactory==0.7.2.dev0)\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.2.1+cu121)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (1.25.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (5.9.5)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.23.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.9.5)\n","Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.2.2)\n","Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.16.3 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading gradio_client-0.16.3-py3-none-any.whl (315 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.1.5)\n","Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (9.4.0)\n","Collecting pydub (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading ruff-0.4.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.11.0)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.0.7)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.3->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (2.8.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (2.18.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (3.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.7.2.dev0)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (0.19.1)\n","Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl>=0.8.1->llamafactory==0.7.2.dev0) (0.8.4)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.7.2.dev0) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn->llamafactory==0.7.2.dev0)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.7.2.dev0)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.7.2.dev0)\n","  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n","Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->llamafactory==0.7.2.dev0)\n","  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->llamafactory==0.7.2.dev0)\n","  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (2.4.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llamafactory==0.7.2.dev0) (3.7.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.12.1)\n","Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0) (3.7)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.0.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (1.3.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.3.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llamafactory==0.7.2.dev0) (1.2.1)\n","Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (13.7.1)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (0.16)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (1.7.1)\n","Collecting httptools>=0.5.0 (from uvicorn->llamafactory==0.7.2.dev0)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->llamafactory==0.7.2.dev0)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.7.2.dev0) (1.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.18.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.1.2)\n","Building wheels for collected packages: fire, llamafactory, ffmpy\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=90526424d80c22944723a02095b81ae00ef00fff81dc939019243616bb538313\n","  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n","  Building wheel for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llamafactory: filename=llamafactory-0.7.2.dev0-py3-none-any.whl size=164478 sha256=dae52adaf388eaaadea7339ca45de216b09d687eedd227eb536f9e068ad04669\n","  Stored in directory: /root/.cache/pip/wheels/c6/10/81/d63b01ba8d9494d49c948978820afc36c7b07d002915649a0d\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=bd2be5e765674cc3e134158b558835b3a4332d8d8c6c910ebaeb885351ebb76f\n","  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n","Successfully built fire llamafactory ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httptools, h11, fire, einops, dnspython, aiofiles, watchfiles, uvicorn, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, typer, sse-starlette, nvidia-cusolver-cu12, httpx, gradio-client, bitsandbytes, accelerate, trl, peft, fastapi-cli, fastapi, gradio, llamafactory\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.4\n","    Uninstalling typer-0.9.4:\n","      Successfully uninstalled typer-0.9.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.30.1 aiofiles-23.2.1 bitsandbytes-0.43.1 dnspython-2.6.1 einops-0.8.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 ffmpy-0.3.2 fire-0.6.0 gradio-4.31.3 gradio-client-0.16.3 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.7.2.dev0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 orjson-3.10.3 peft-0.11.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.4 semantic-version-2.10.0 shellingham-1.5.4 sse-starlette-2.1.0 starlette-0.37.2 tomlkit-0.12.0 trl-0.8.6 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"]}],"source":["# %rm -rf LLaMA-Factory\n","# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n","%cd ../LLaMA-Factory\n","%ls\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps xformers<0.0.26\n","!pip install .[torch,bitsandbytes]"]},{"cell_type":"markdown","source":["### Check GPU environment"],"metadata":{"id":"H9RXn_YQnn9f"}},{"cell_type":"code","source":["import torch\n","try:\n","  assert torch.cuda.is_available() is True\n","except AssertionError:\n","  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"],"metadata":{"id":"ZkN-ktlsnrdU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Update Identity Dataset"],"metadata":{"id":"TeYs5Lz-QJYk"}},{"cell_type":"code","source":["%ls data"],"metadata":{"id":"PMjxwnzFjw1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","\n","\n","NAME = \"Llama-3\"\n","AUTHOR = \"LLaMA Factory\"\n","\n","with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n","  dataset = json.load(f)\n","\n","for sample in dataset:\n","  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n","\n","with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n","  json.dump(dataset, f, indent=2, ensure_ascii=False)"],"metadata":{"id":"ap_fvMBsQHJc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune model via LLaMA Board"],"metadata":{"id":"2QiXcvdzzW3Y"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wadLY8ec2ikv","executionInfo":{"status":"ok","timestamp":1715889814618,"user_tz":-120,"elapsed":12876,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"e79ef50e-4d44-461f-a412-8adcf1861f8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["\n","!GRADIO_SHARE=1 llamafactory-cli webui"],"metadata":{"id":"YLsdS6V5yUMy","outputId":"11698440-5fe8-47f1-87eb-2aca40576db5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715893223174,"user_tz":-120,"elapsed":621284,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-05-16 20:50:07.369252: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-16 20:50:07.369304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-16 20:50:07.370640: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-16 20:50:08.817470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Running on local URL:  http://0.0.0.0:7860\n","Running on public URL: https://a1a48dd5054bbc0727.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","2024-05-16 20:52:25.179818: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-16 20:52:25.179864: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-16 20:52:25.181140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-16 20:52:26.388426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/16/2024 20:52:30 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 20:52:30,368 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 20:52:30,369 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 20:52:30,369 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 20:52:30,369 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n","[WARNING|logging.py:314] 2024-05-16 20:52:31,032 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","05/16/2024 20:52:31 - INFO - llamafactory.data.loader - Loading dataset oppositional_split_train_en.json...\n","/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 114.04 examples/s]\n","Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:08<00:00, 11.22 examples/s]\n","input_ids:\n","[35075, 25, 9587, 287, 264, 1495, 439, 22236, 39036, 994, 433, 374, 11, 304, 2144, 11, 16632, 14076, 278, 311, 21391, 6325, 11, 1436, 13893, 3063, 1884, 889, 1051, 5042, 10371, 4860, 12401, 311, 26359, 10977, 13, 1952, 279, 1023, 1450, 11, 304, 279, 2317, 315, 279, 20562, 12, 777, 34606, 8274, 11, 25607, 22236, 39036, 2262, 430, 16696, 311, 4124, 279, 28522, 477, 586, 2890, 11429, 439, 264, 1121, 315, 264, 8762, 72137, 26359, 555, 6367, 32549, 5315, 374, 3062, 13, 56877, 420, 11, 1053, 499, 2440, 420, 439, 22236, 39036, 5380, 29690, 279, 779, 2663, 330, 52830, 2469, 330, 902, 6137, 439, 279, 330, 7295, 52830, 2469, 330, 949, 1102, 3952, 2035, 927, 220, 605, 1667, 4227, 323, 279, 3109, 5535, 55223, 311, 46403, 20021, 323, 6012, 369, 2919, 389, 842, 14718, 11990, 315, 16701, 5922, 315, 5674, 662, 578, 892, 994, 279, 5961, 55671, 9495, 1523, 323, 9977, 6244, 63677, 1405, 55223, 7020, 279, 10289, 1051, 330, 41398, 330, 323, 682, 505, 3424, 311, 10632, 6244, 6762, 1847, 5354, 2209, 308, 956, 433, 15234, 420, 78360, 279, 1890, 3109, 902, 10187, 279, 8013, 586, 304, 8818, 315, 40300, 1174, 27227, 323, 5042, 9564, 279, 3838, 662, 1115, 1890, 1887, 902, 264, 2478, 55223, 14132, 1047, 304, 22743, 1174, 1457, 706, 279, 5070, 311, 617, 279, 5961, 6699, 53731, 311, 3488, 95904, 58378, 304, 8853, 662, 578, 6625, 380, 17942, 315, 63667, 2955, 374, 1629, 555, 17047, 889, 3412, 10973, 2410, 520, 682, 3115, 1174, 2646, 387, 80647, 555, 5606, 889, 10975, 499, 6062, 662, 9062, 315, 279, 99870, 323, 2875, 482, 17503, 584, 3663, 3432, 2586, 505, 1884, 889, 364, 588, 1027, 51920, 311, 6144, 603, 662, 1781, 369, 6261, 720, 72803, 25, 220, 5910, 40879, 49, 44845, 128009]\n","inputs:\n","Human: Labeling a text as conspiratorial when it is, in fact, merely oppositional to mainstream views, could potentially lead those who were simply asking questions closer to conspiracy communities. On the other hand, in the context of the COVID-19 Pandemic, identifying conspiratorial content that tries to frame the pandemic or public health decisions as a result of a malevolent conspiracy by secret influential groups is important. Considering this, would you label this as conspiratorial?\n","Remember the so called \" Riots \" which began as the \" London Riots \"? It took place over 10 years ago and the government allowed gangs to loot shops and properties for days on end causing millions of pounds worth of damage. The time when the countries policing shut down and crime became rampant where gangs knew the Police were \" stretched \" and all from property to homes became fair game.. Isn't it strange this rocked the same government which holds the British public in terror of vaccines, masks and simply leaving the house. This same system which a few gangs apparently had in panic, now has the resources to have the countries parents terrified to question perverse narratives in schools. The internationalist regime of sinister design is run by governments who hold absolute power at all times, never be fooled by anyone who tells you otherwise. Each of the tragedies and short - falls we face today come from those who've been tasked to protect us. think for yourself \n","Assistant: CONSPIRACY<|eot_id|>\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5910, 40879, 49, 44845, 128009]\n","labels:\n","CONSPIRACY<|eot_id|>\n","[INFO|configuration_utils.py:726] 2024-05-16 20:52:42,730 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n","[INFO|configuration_utils.py:789] 2024-05-16 20:52:42,732 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","05/16/2024 20:52:42 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n","[WARNING|quantization_config.py:282] 2024-05-16 20:52:42,868 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","[INFO|modeling_utils.py:3429] 2024-05-16 20:52:42,872 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n","[INFO|modeling_utils.py:1494] 2024-05-16 20:52:42,922 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:928] 2024-05-16 20:52:42,930 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009\n","}\n","\n","[INFO|modeling_utils.py:4170] 2024-05-16 20:52:55,472 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4178] 2024-05-16 20:52:55,473 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:883] 2024-05-16 20:52:55,527 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n","[INFO|configuration_utils.py:928] 2024-05-16 20:52:55,528 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128009\n","  ]\n","}\n","\n","05/16/2024 20:52:55 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n","05/16/2024 20:52:55 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n","05/16/2024 20:52:55 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n","05/16/2024 20:52:55 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n","05/16/2024 20:52:56 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 8033669120 || trainable%: 0.0424\n","[INFO|trainer.py:626] 2024-05-16 20:52:56,185 >> Using auto half precision backend\n","[INFO|trainer.py:2048] 2024-05-16 20:52:56,582 >> ***** Running training *****\n","[INFO|trainer.py:2049] 2024-05-16 20:52:56,582 >>   Num examples = 90\n","[INFO|trainer.py:2050] 2024-05-16 20:52:56,582 >>   Num Epochs = 3\n","[INFO|trainer.py:2051] 2024-05-16 20:52:56,582 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2054] 2024-05-16 20:52:56,582 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2055] 2024-05-16 20:52:56,582 >>   Gradient Accumulation steps = 4\n","[INFO|trainer.py:2056] 2024-05-16 20:52:56,582 >>   Total optimization steps = 15\n","[INFO|trainer.py:2057] 2024-05-16 20:52:56,586 >>   Number of trainable parameters = 3,407,872\n"," 33% 5/15 [01:35<03:38, 21.83s/it]05/16/2024 20:54:31 - INFO - llamafactory.extras.callbacks - {'loss': 9.6940, 'learning_rate': 3.7500e-05, 'epoch': 0.87}\n","{'loss': 9.694, 'grad_norm': 11.396599769592285, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.87}\n"," 67% 10/15 [03:03<01:35, 19.03s/it]05/16/2024 20:55:59 - INFO - llamafactory.extras.callbacks - {'loss': 8.5180, 'learning_rate': 1.2500e-05, 'epoch': 1.74}\n","{'loss': 8.518, 'grad_norm': 12.140657424926758, 'learning_rate': 1.2500000000000006e-05, 'epoch': 1.74}\n"," 67% 10/15 [03:03<01:35, 19.03s/it][INFO|trainer.py:3614] 2024-05-16 20:55:59,845 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3616] 2024-05-16 20:55:59,845 >>   Num examples = 10\n","[INFO|trainer.py:3619] 2024-05-16 20:55:59,845 >>   Batch size = 4\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n"," 67% 2/3 [00:01<00:00,  1.91it/s]\u001b[A\n","                                   \n","\u001b[A{'eval_loss': 8.533793449401855, 'eval_runtime': 5.613, 'eval_samples_per_second': 1.782, 'eval_steps_per_second': 0.534, 'epoch': 1.74}\n"," 67% 10/15 [03:08<01:35, 19.03s/it]\n","100% 3/3 [00:01<00:00,  1.72it/s]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:3305] 2024-05-16 20:56:05,463 >> Saving model checkpoint to saves/Custom/lora/train_2024-05-16-20-50-22/checkpoint-10\n","[INFO|configuration_utils.py:726] 2024-05-16 20:56:05,682 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n","[INFO|configuration_utils.py:789] 2024-05-16 20:56:05,683 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2488] 2024-05-16 20:56:05,783 >> tokenizer config file saved in saves/Custom/lora/train_2024-05-16-20-50-22/checkpoint-10/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2497] 2024-05-16 20:56:05,790 >> Special tokens file saved in saves/Custom/lora/train_2024-05-16-20-50-22/checkpoint-10/special_tokens_map.json\n","100% 15/15 [04:38<00:00, 19.08s/it]05/16/2024 20:57:34 - INFO - llamafactory.extras.callbacks - {'loss': 7.9329, 'learning_rate': 5.4631e-07, 'epoch': 2.61}\n","{'loss': 7.9329, 'grad_norm': 12.069125175476074, 'learning_rate': 5.463099816548579e-07, 'epoch': 2.61}\n","100% 15/15 [04:38<00:00, 19.08s/it][INFO|trainer.py:2316] 2024-05-16 20:57:34,943 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2521] 2024-05-16 20:57:34,943 >> Loading best model from saves/Custom/lora/train_2024-05-16-20-50-22/checkpoint-10 (score: 8.533793449401855).\n","{'train_runtime': 278.4405, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.054, 'train_loss': 8.714956410725911, 'epoch': 2.61}\n","100% 15/15 [04:38<00:00, 18.56s/it]\n","[INFO|trainer.py:3305] 2024-05-16 20:57:35,032 >> Saving model checkpoint to saves/Custom/lora/train_2024-05-16-20-50-22\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|configuration_utils.py:726] 2024-05-16 20:57:35,195 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n","[INFO|configuration_utils.py:789] 2024-05-16 20:57:35,196 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2488] 2024-05-16 20:57:35,283 >> tokenizer config file saved in saves/Custom/lora/train_2024-05-16-20-50-22/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2497] 2024-05-16 20:57:35,289 >> Special tokens file saved in saves/Custom/lora/train_2024-05-16-20-50-22/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =     2.6087\n","  total_flos               =  3538407GF\n","  train_loss               =      8.715\n","  train_runtime            = 0:04:38.44\n","  train_samples_per_second =       0.97\n","  train_steps_per_second   =      0.054\n","Figure saved at: saves/Custom/lora/train_2024-05-16-20-50-22/training_loss.png\n","Figure saved at: saves/Custom/lora/train_2024-05-16-20-50-22/training_eval_loss.png\n","[INFO|trainer.py:3614] 2024-05-16 20:57:35,924 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3616] 2024-05-16 20:57:35,924 >>   Num examples = 10\n","[INFO|trainer.py:3619] 2024-05-16 20:57:35,924 >>   Batch size = 4\n","100% 3/3 [00:01<00:00,  1.77it/s]\n","***** eval metrics *****\n","  epoch                   =     2.6087\n","  eval_loss               =     8.5338\n","  eval_runtime            = 0:00:05.36\n","  eval_samples_per_second =      1.865\n","  eval_steps_per_second   =       0.56\n","[INFO|modelcard.py:450] 2024-05-16 20:57:41,305 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","Keyboard interruption in main thread... closing server.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2619, in block_thread\n","    time.sleep(0.1)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/cli.py\", line 69, in main\n","    run_web_ui()\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/webui/interface.py\", line 76, in run_web_ui\n","    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2524, in launch\n","    self.block_thread()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2623, in block_thread\n","    self.server.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 68, in close\n","    self.thread.join(timeout=5)\n","  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n","    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","Killing tunnel 0.0.0.0:7860 <> https://a1a48dd5054bbc0727.gradio.live\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkGOC14lgK4H","executionInfo":{"status":"ok","timestamp":1715881442767,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"dab238f7-c585-4ee4-9b27-6e082963637c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34massets\u001b[0m/       docker-compose.yml  \u001b[01;34mexamples\u001b[0m/  pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n","CITATION.cff  Dockerfile          LICENSE    README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/         \u001b[01;34mevaluation\u001b[0m/         Makefile   README_zh.md    setup.py\n"]}]},{"cell_type":"markdown","source":["## Fine-tune model via Command Line\n","\n","It takes ~30min for training."],"metadata":{"id":"rgR3UFhB0Ifq"}},{"cell_type":"code","source":["import json\n","\n","args = dict(\n","  stage=\"sft\",                        # do supervised fine-tuning\n","  do_train=True,\n","  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n","  dataset=\"llama_instruct_dataset_test_en\",             # use alpaca and identity datasets\n","  template=\"llama3\",                     # use llama3 prompt template\n","  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n","  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n","  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n","  per_device_train_batch_size=2,               # the batch size\n","  gradient_accumulation_steps=4,               # the gradient accumulation steps\n","  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n","  logging_steps=10,                      # log every 10 steps\n","  warmup_ratio=0.1,                      # use warmup scheduler\n","  save_steps=1000,                      # save checkpoint every 1000 steps\n","  learning_rate=5e-5,                     # the learning rate\n","  num_train_epochs=3.0,                    # the epochs of training\n","  max_samples=500,                      # use 500 examples in each dataset\n","  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n","  quantization_bit=4,                     # use 4-bit QLoRA\n","  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n","  use_unsloth=True,                      # use UnslothAI's LoRA optimization for 2x faster training\n","  fp16=True,                         # use float16 mixed precision training\n",")\n","\n","json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n","\n","%cd /content/LLaMA-Factory/\n","\n","!llamafactory-cli train train_llama3.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CS0Qk5OR0i4Q","executionInfo":{"status":"ok","timestamp":1715884648155,"user_tz":-120,"elapsed":18650,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"de88a347-d821-4c48-8236-6c3fbb9517cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/LLaMA-Factory/'\n","/content/drive/MyDrive/NLP_2_Conspiracies/code/task/LM_classification_huggingface/fine_tune_llama/LLaMA-Factory\n","2024-05-16 18:37:15.803435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-16 18:37:15.803480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-16 18:37:15.805009: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-16 18:37:17.205353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/16/2024 18:37:22 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n","05/16/2024 18:37:22 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 18:37:22,716 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45/tokenizer.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 18:37:22,716 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 18:37:22,716 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 18:37:22,717 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45/tokenizer_config.json\n","[WARNING|logging.py:314] 2024-05-16 18:37:23,444 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","05/16/2024 18:37:23 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n","05/16/2024 18:37:23 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n","05/16/2024 18:37:23 - INFO - llamafactory.data.loader - Loading dataset llama_instruct_dataset_test_en.json...\n","Converting format of dataset: 100% 10/10 [00:00<00:00, 684.80 examples/s]\n","Running tokenizer on dataset: 100% 10/10 [00:00<00:00, 473.39 examples/s]\n","input_ids:\n","[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 2535, 287, 264, 1495, 439, 22236, 39036, 994, 433, 374, 11, 304, 2144, 11, 16632, 14076, 278, 311, 21391, 6325, 11, 1436, 13893, 3063, 1884, 889, 1051, 5042, 10371, 4860, 12401, 311, 26359, 10977, 13, 1952, 279, 1023, 1450, 11, 304, 279, 2317, 315, 279, 20562, 12, 777, 34606, 8274, 11, 25607, 22236, 39036, 2262, 430, 16696, 311, 4124, 279, 28522, 477, 586, 2890, 11429, 439, 264, 1121, 315, 264, 8762, 72137, 26359, 555, 6367, 32549, 5315, 374, 3062, 13, 56877, 420, 11, 1053, 499, 2440, 420, 439, 22236, 39036, 5380, 29690, 279, 779, 2663, 330, 52830, 2469, 330, 902, 6137, 439, 279, 330, 7295, 52830, 2469, 330, 949, 1102, 3952, 2035, 927, 220, 605, 1667, 4227, 323, 279, 3109, 5535, 55223, 311, 46403, 20021, 323, 6012, 369, 2919, 389, 842, 14718, 11990, 315, 16701, 5922, 315, 5674, 662, 578, 892, 994, 279, 5961, 55671, 9495, 1523, 323, 9977, 6244, 63677, 1405, 55223, 7020, 279, 10289, 1051, 330, 41398, 330, 323, 682, 505, 3424, 311, 10632, 6244, 6762, 1847, 5354, 2209, 308, 956, 433, 15234, 420, 78360, 279, 1890, 3109, 902, 10187, 279, 8013, 586, 304, 8818, 315, 40300, 1174, 27227, 323, 5042, 9564, 279, 3838, 662, 1115, 1890, 1887, 902, 264, 2478, 55223, 14132, 1047, 304, 22743, 1174, 1457, 706, 279, 5070, 311, 617, 279, 5961, 6699, 53731, 311, 3488, 95904, 58378, 304, 8853, 662, 578, 6625, 380, 17942, 315, 63667, 2955, 374, 1629, 555, 17047, 889, 3412, 10973, 2410, 520, 682, 3115, 1174, 2646, 387, 80647, 555, 5606, 889, 10975, 499, 6062, 662, 9062, 315, 279, 99870, 323, 2875, 482, 17503, 584, 3663, 3432, 2586, 505, 1884, 889, 364, 588, 1027, 51920, 311, 6144, 603, 662, 1781, 369, 6261, 220, 128009, 128006, 78191, 128007, 271, 5910, 40879, 49, 44845, 128009]\n","inputs:\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","Labeling a text as conspiratorial when it is, in fact, merely oppositional to mainstream views, could potentially lead those who were simply asking questions closer to conspiracy communities. On the other hand, in the context of the COVID-19 Pandemic, identifying conspiratorial content that tries to frame the pandemic or public health decisions as a result of a malevolent conspiracy by secret influential groups is important. Considering this, would you label this as conspiratorial?\n","Remember the so called \" Riots \" which began as the \" London Riots \"? It took place over 10 years ago and the government allowed gangs to loot shops and properties for days on end causing millions of pounds worth of damage. The time when the countries policing shut down and crime became rampant where gangs knew the Police were \" stretched \" and all from property to homes became fair game.. Isn't it strange this rocked the same government which holds the British public in terror of vaccines, masks and simply leaving the house. This same system which a few gangs apparently had in panic, now has the resources to have the countries parents terrified to question perverse narratives in schools. The internationalist regime of sinister design is run by governments who hold absolute power at all times, never be fooled by anyone who tells you otherwise. Each of the tragedies and short - falls we face today come from those who've been tasked to protect us. think for yourself <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","CONSPIRACY<|eot_id|>\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5910, 40879, 49, 44845, 128009]\n","labels:\n","CONSPIRACY<|eot_id|>\n","[INFO|configuration_utils.py:726] 2024-05-16 18:37:24,736 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45/config.json\n","[INFO|configuration_utils.py:789] 2024-05-16 18:37:24,737 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","05/16/2024 18:37:24 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/cli.py\", line 65, in main\n","    run_exp()\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/train/tuner.py\", line 33, in run_exp\n","    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/train/sft/workflow.py\", line 34, in run_sft\n","    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/model/loader.py\", line 124, in load_model\n","    model = load_unsloth_pretrained_model(config, model_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/model/utils/unsloth.py\", line 39, in load_unsloth_pretrained_model\n","    from unsloth import FastLanguageModel\n","  File \"/usr/local/lib/python3.10/dist-packages/unsloth/__init__.py\", line 113, in <module>\n","    from .models import *\n","  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/__init__.py\", line 15, in <module>\n","    from .loader  import FastLanguageModel\n","  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py\", line 15, in <module>\n","    from .llama import FastLlamaModel, logger\n","  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\", line 27, in <module>\n","    from ._utils import *\n","  File \"/usr/local/lib/python3.10/dist-packages/unsloth/models/_utils.py\", line 60, in <module>\n","    import xformers.ops.fmha as xformers\n","ModuleNotFoundError: No module named 'xformers'\n"]}]},{"cell_type":"markdown","source":["## Infer the fine-tuned model"],"metadata":{"id":"PVNaC-xS5N40"}},{"cell_type":"code","source":["from llamafactory.chat import ChatModel\n","from llamafactory.extras.misc import torch_gc\n","\n","%cd /content/LLaMA-Factory/\n","\n","args = dict(\n","  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n","  adapter_name_or_path=\"saves/Custom/lora/toy2\",  # load the saved LoRA adapters\n","  template=\"llama3\",                     # same to the one in training\n","  finetuning_type=\"lora\",                  # same to the one in training\n","  quantization_bit=4,                    # load 4-bit quantized model\n","  # use_unsloth=True,                     # use UnslothAI's LoRA optimization for 2x faster generation\n",")\n","chat_model = ChatModel(args)\n","\n","messages = []\n","print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n","while True:\n","  query = input(\"\\nUser: \")\n","  if query.strip() == \"exit\":\n","    break\n","  if query.strip() == \"clear\":\n","    messages = []\n","    torch_gc()\n","    print(\"History has been removed.\")\n","    continue\n","\n","  messages.append({\"role\": \"user\", \"content\": query})\n","  print(\"Assistant: \", end=\"\", flush=True)\n","\n","  response = \"\"\n","  for new_text in chat_model.stream_chat(messages):\n","    print(new_text, end=\"\", flush=True)\n","    response += new_text\n","  print()\n","  messages.append({\"role\": \"assistant\", \"content\": response})\n","\n","torch_gc()"],"metadata":{"id":"oh8H9A_25SF9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8d64161-895d-464f-c55c-dc135594150a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/LLaMA-Factory/'\n","/content/drive/MyDrive/NLP_2_Conspiracies/code/task/LM_classification_huggingface/fine_tune_llama/LLaMA-Factory\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:47:11,843 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:47:11,844 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:47:11,847 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:47:11,852 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n","[WARNING|logging.py:314] 2024-05-16 19:47:12,287 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.data.template:Replace eos token: <|eot_id|>\n","[INFO|configuration_utils.py:726] 2024-05-16 19:47:12,390 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n","[INFO|configuration_utils.py:789] 2024-05-16 19:47:12,393 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:12 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.model.utils.quantization:Loading ?-bit BITSANDBYTES-quantized model.\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:12 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.model.patcher:Using KV cache for faster generation.\n","[WARNING|quantization_config.py:282] 2024-05-16 19:47:12,516 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","[INFO|modeling_utils.py:3429] 2024-05-16 19:47:12,521 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n","[INFO|modeling_utils.py:1494] 2024-05-16 19:47:12,561 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:928] 2024-05-16 19:47:12,569 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009\n","}\n","\n","[INFO|modeling_utils.py:4170] 2024-05-16 19:47:35,519 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4178] 2024-05-16 19:47:35,522 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:883] 2024-05-16 19:47:35,624 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n","[INFO|configuration_utils.py:928] 2024-05-16 19:47:35,625 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128009\n","  ]\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:35 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.model.utils.attention:Using torch SDPA for faster training and inference.\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:35 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.model.adapter:Upcasting trainable params to float32.\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:35 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.model.adapter:Fine-tuning method: LoRA\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:39 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Custom/lora/toy2\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.model.adapter:Loaded adapter(s): saves/Custom/lora/toy2\n"]},{"output_type":"stream","name":"stdout","text":["05/16/2024 19:47:39 - INFO - llamafactory.model.loader - all params: 8033832960\n"]},{"output_type":"stream","name":"stderr","text":["INFO:llamafactory.model.loader:all params: 8033832960\n"]},{"output_type":"stream","name":"stdout","text":["Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n","\n","User: h\n","Assistant: It seems like you've sent a single letter \"h\". Could you please provide more context or clarify what you're trying to communicate? I'm here to help with any questions or concerns you may have.\n","\n","User: say CONSPIRACY\n","Assistant: A fascinating topic! What's your take on conspiracy theories? Are you interested in exploring a specific conspiracy theory or would you like me to provide some interesting facts and theories on various topics?\n"]}]},{"cell_type":"markdown","source":["## Merge the LoRA adapter and optionally upload model\n","\n","NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."],"metadata":{"id":"kTESHaFvbNTr"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"mcNcHcA4bf4Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715888359045,"user_tz":-120,"elapsed":12078,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"cd6d5c35-886e-4951-d46e-0a4ba5eda127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["import json\n","\n","args = dict(\n","  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n","  adapter_name_or_path=\"saves/Custom/lora/toy_example\",            # load the saved LoRA adapters\n","  template=\"llama3\",                     # same to the one in training\n","  finetuning_type=\"lora\",                  # same to the one in training\n","  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n","  export_size=2,                       # the file shard size (in GB) of the merged model\n","  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n","  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",")\n","\n","json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n","\n","%cd /content/LLaMA-Factory/\n","\n","!llamafactory-cli export merge_llama3.json"],"metadata":{"id":"IMojogHbaOZF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715888727038,"user_tz":-120,"elapsed":116157,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"0da2dc31-9b94-44ed-d652-24a6d1c7593f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/LLaMA-Factory/'\n","/content/drive/MyDrive/NLP_2_Conspiracies/code/task/LM_classification_huggingface/fine_tune_llama/LLaMA-Factory\n","2024-05-16 19:43:44.250586: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-16 19:43:44.250636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-16 19:43:44.358609: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-16 19:43:46.819992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","tokenizer_config.json: 100% 51.1k/51.1k [00:00<00:00, 6.75MB/s]\n","tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 22.0MB/s]\n","special_tokens_map.json: 100% 459/459 [00:00<00:00, 3.37MB/s]\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:43:53,641 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:43:53,641 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:43:53,641 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-16 19:43:53,641 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n","[WARNING|logging.py:314] 2024-05-16 19:43:54,025 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","05/16/2024 19:43:54 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n","config.json: 100% 1.15k/1.15k [00:00<00:00, 7.38MB/s]\n","[INFO|configuration_utils.py:726] 2024-05-16 19:43:54,187 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n","[INFO|configuration_utils.py:789] 2024-05-16 19:43:54,188 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","05/16/2024 19:43:54 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n","05/16/2024 19:43:54 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n","[WARNING|quantization_config.py:282] 2024-05-16 19:43:54,385 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","model.safetensors: 100% 5.70G/5.70G [00:58<00:00, 96.9MB/s]\n","[INFO|modeling_utils.py:3429] 2024-05-16 19:44:53,521 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n","[INFO|modeling_utils.py:1494] 2024-05-16 19:44:53,631 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:928] 2024-05-16 19:44:53,635 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128009\n","}\n","\n","[INFO|modeling_utils.py:4170] 2024-05-16 19:45:19,206 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4178] 2024-05-16 19:45:19,206 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","generation_config.json: 100% 131/131 [00:00<00:00, 865kB/s]\n","[INFO|configuration_utils.py:883] 2024-05-16 19:45:19,373 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n","[INFO|configuration_utils.py:928] 2024-05-16 19:45:19,373 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128009\n","  ]\n","}\n","\n","05/16/2024 19:45:19 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n","05/16/2024 19:45:19 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n","05/16/2024 19:45:19 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n","05/16/2024 19:45:23 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Custom/lora/toy_example\n","05/16/2024 19:45:23 - INFO - llamafactory.model.loader - all params: 8033832960\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/cli.py\", line 63, in main\n","    export_model()\n","  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/train/tuner.py\", line 62, in export_model\n","    raise ValueError(\"Cannot merge adapters to a quantized model.\")\n","ValueError: Cannot merge adapters to a quantized model.\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-xKT1X80QZZ","executionInfo":{"status":"ok","timestamp":1715886695290,"user_tz":-120,"elapsed":719,"user":{"displayName":"Nikolaj Bauer","userId":"03171721548776402258"}},"outputId":"7870951a-55f3-457c-e7fd-ec2bc6041971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdata\u001b[0m/               LICENSE            README_zh.md      \u001b[01;34msrc\u001b[0m/\n","\u001b[01;34mbuild\u001b[0m/        docker-compose.yml  Makefile           requirements.txt  \u001b[01;34mtests\u001b[0m/\n","\u001b[01;34mcache\u001b[0m/        Dockerfile          merge_llama3.json  \u001b[01;34msaves\u001b[0m/            train_llama3.json\n","CITATION.cff  \u001b[01;34mevaluation\u001b[0m/         pyproject.toml     \u001b[01;34mscripts\u001b[0m/\n","\u001b[01;34mconfig\u001b[0m/       \u001b[01;34mexamples\u001b[0m/           README.md          setup.py\n"]}]}]}